cf push app-name -p ./path -m memoryM/G --random-route
Ephemeral infrastructure

---virtual machines and containers are temporary

Immutable infrastructure

---system and application updates are not done on existing apps and systems
but new updated instances are created

12 factor apps
---------------
1)processes
2)concurrency
3)disposability
4)logs

Processes are stateless so don't store it in app or sys instances.
we will persist the state to some backing services

concurrency--> via the process model.workers can read from a message queue or web apps which might serve our customers

disposability--> fast start up and graceful shutdown

Logs--> treated as event streams. app don't need to manage logs. system will determine
what happens to the logs


Elastic runtime architecture
---------------------------------------
subsystems
----> diego, loggregator, cloud controller api, routing


Diego
-----------
schedular-> schedule tasks and long running processes(LRPs)

task-> event which is guaranteed to run atleast once
example-> staging an application

Lrps-> an web app

Diego container->app instances are run within a container which is immutable
Diego cell-> container runs within a cell and there can be many containers depending on the
no of app instances
pcf has pool of cells

Diego Garden--> is an interface which manages the containers
Has many backends. example linux and windows
Rep is a process that is running on a cell. and represents the cell in auctions
Auction is held to bid on executing a task and lrp
Executor is a subprocess of Rep.and provides what going on with the cell.
Example does the cell has more memory to create one for container for an app instance

Executor forwards logs to metron which forwards it to a loggregator
Metron is present in the cell
BBS-->Bulletin Board System API--> to access Diego DB for tasks and Lrp
persists the information to the ectd db

Brain-> holds the auction
Auctioneer, converger --> subprocess


Loggregator
--------------------
metron passes logging info to loggregator sub system

sub sys has different components

Doppler--> ability to create app syslog drain-> third party 
Traffic controller->> handling requests--> cf logs
		  -->> also exposes websocket endpoint called firehose
firehose-> app logs, container metric elr component metrics
	--> don't include Er component logs
metron --doppler --- traffic controller --- datadog nozzle

nozzles
consumes firehose output


Cloud controller 

exposes api for using and managing Elastic runtime

cloud controller persits its info in Cloud Controller Database

Blob store --> when we upload it persists the app artifact id to the blobstore

	droplet gets saved to blobstore


CC --> CC-bridge--> BBS
cc-bridge--> app specific domain to generic diego language of tasks and LRPS

Routing
------------------------------------
it maps the req to appropriate app instances running on given cells

maps cf push to CC

FLows
---------------------
Staging

cc --> cc-bridge--> BBS(bulletin board sys)-->Brain(auctioneer)->> cells--reps(executor)->>container

Generic Domain Tasks and LRPs



Buildpacks
---------------------------------
responsible for building the droplet and is immutable (provide the runtime environment)


High Availability
----------------------------
instances are distributed to availability zones

Bosh managed process
------------------
monitoring of elastic runtime processes and automatically restarted

Monit and Bosh agent
CC Vm contains the cc processes as well as monit and bosh agents

Monit restarts the CC process and pass rthe info to the Bosh agent which will report it to Bosh

Bosh is a vm is managing the entire distributed system

bosh agent --> message bus --> health monitor informs via mails and other stuff

if router vm or cc vm fails the health monitor resurrector plugin will understand there is a missing vm
and ask the Bosh director to provision one.

Self healing app instances
------------------------------------------
rep reports states of the given to bbs and the brain will look at it and compares with the expected state and then decide
to hold an auction for provision



Commands for Logs
-------------------------------------
cf logs app-name
cf start app-name

cf logs app-name | grep "API\|CELL" --> to remove unnecessary logs 

to dedicate more memory-->> cf scale app-name -m 1G

to display health----> cf app app-name

to scale out as in increase the number of instances--> horizontal scaling --> cf scale app-name -i 3
**no downtime associated with scaling horizontally

cf events app-name--> to provide the data regarding various deployment events

cf app app-name --> gives the data of the application that is running. like instances, memory, state etc


For service
-----------------------------------
cf create-service service-name plan-name custom-service-instance-name

cf bind-service app-name service-instance-name

VCAP_SERVICES and inject it into the app container so that the app can parse it and connect to the db

the details of the service instance get put into our app env by env variable called VCAP_SERVICES

cf env service-name


User Provide Service Instance
---------------------------------------------------
Services outside of the marketplace of PCF

cf create-user-provided-service service-instance-name -p uri

Manage service instance lifecycle 
--> creating, binding, unbinding, deleting


A service instance is a reserved resource-> example database on shared or dedicated system or vm

VCAP_SERVICES helps in exposing configuration stuff to our application which is stored in the environment
through the VCAP_SERVICES variable

managed service - present in marketplace
user provided service instance --> not present in marketplace. provisioned out side the pcf

****EXAMPLE****

cf create-service cleardb spark attendee-mysql

cf bind-service attendee-service attendee-mysql

cf create-user-provided-service attendee-service -p uri
uri--> articulate application uri

cf bind-service articulat attendee-service

Manifest
------------------------------------------------------------
By default if there is a manifest.yml file on the same path then cli(cf push) will automatically parse it...
To avoid, remove the manifest
file from the path



To create a manifest file

-------------------------------------------------------------

cf create-app-manifest attendee-service -p ./manifest.yml


it helps in specifying everything in file rather than on the command line

Manifest is not uploaded with the application. It is parsed by the cli

Cli sends multiple rest requests to the cloud controller

For services to bind to the application it needs to be in the manifest file. else it will
not be bound to the application.


cf set env --> to set an environment variable




Application Security Group Module
------------------------------------------------------------------------------
app sec group are firewalls that outbound traffic for apps
Example -> from app to db

Allows us to decide which security policy to be implemented during various lifecycle of 
the app deployment

Example--> during staging we can not allow the app to access our db but allow it to download 
various dependencies

space security groups
-----------------------------
applicable to app running within a given space which can be combined with running security
group for outbound traffic from the app

security group rules file
---------------------------------------
json based
1) protocol
2) destination
3) ports

cf t --> user profile

cf security-groups --> to see security groups that are running and are bound to my dev space

cf security-group security-group-name --> info about the particular security group


staging and running security groups are system wide. They apply everywhere.

Single security group definition can be applied to different spaces.

cf staging-security-groups, cf running-security-groups ----> to get info about security groups 
attached to various lifecycles.

cf unbind-running-security-group security-group-name ---> to unbind the security group attached to 
															application ---> can only be done 
																			by the admin
																			
****changes takes place when the existing running applications are restarted.

cf create-security-group group-instance-name ./pathname

to bind a specific security group 
-----------------------------------------------
cf bind-security-group group-instance-name	org-name space-name
example:- cf bind-security-group rambo sauveek development

***whitelist is used for implementing security groups																	

relationship between running and space security groups
--->union of both of them put together. if allowed anywhere, the application will be able to 
take advantage of that

Why security groups
----------------------------
protection against typically trusted resources like employees
limit your exposure. force app to use appropriate application or api


LOGS
-------------------------------------------------------------
Doppler--> responsible for importing the logs onto the different log drains

Traffic Controller-->api for the command cf logs app-name
			also exposes a websocket endpoint named the firehose

Firehose provides app logs, container metrics and ER component metrics
		Doesn't include ER component logs

Firehose gets consumed through nozzles which are used to implement different 
visualizations to make us understand what is going on with our pcf environment

Command for log drain service
--------------------------------
cf create-user-provided-service articulate-log-drain -l syslog://uri

cf bind-service app-name service log-drain-service

Recap
--------------------------------
cf restage and cf restart is not neccessary for log-drain-service to start working
 

Blue Green Deployments

-----------------------------------------------------------------------

Zero downtime between upgrade or downgrade



Few things we need to take care before using blue green deployements

1)If using serializing objects don't make destructive changes. Example don't remove fields, do have a serialVersionUID
	

Another example is if we don't want to have few fields anymore in our upgraded application then we don't delete the fields 
but we will not reference it any more
	

serialVersionUID allows us to have multiple versions of the object persisted and have our application respond to that
	


Admin Processes

----------------------

One off process example migrating data--> what we need to do is to run the new version in the same environment as the old one


Database

-----------------------------------

Make the changes idempotent-->example-> copy data to new fields but don't delete the data(column) 
Repeat the pattern which can 
only be done if it is idempotent



Never make any destructive changes to the database-->never delete any column



We need our upgraded application to be backwards compatible ---> we can do that by using default values or nullable fields.
****If newly added fields are non-nullable then some default values has to be passed.



Commands

----------------------------------
cf routes ----> to get all the routes within the space

cf push app-name-v2 -p ./jar-name -m memoryM/G -n parent-hostname-temp --no-start
****here -n means a subdomain

cf bind-service app-name-v2 service-instance-name

cf start app-name-v2

To map v2 route to the production route 
---------------------------------------------------
cf map-route app-name-v2 app-name-route-domain.com -n hostname

****example---> cf map-route articulate-v2 gmail.com -n sauveeksen
		first argument is the domain name and the second is the sub domain name or 
		the hostname.
											
How the route gets divided?? --> total traffic/no of instances bound to the route 

To slowly move out the original app instance
---------------------------------------------------- 
scale down the original app-instances and scale up the v2 app-instances

To totally phase out the original app
-------------------------------------------------------
cf unmap-route original-app-name domain -n hostname

and then unmap the temp route from the new version

cf unmap-route v2-app-name domain -n hostname-temp

**Backwards compatibility and non destructive changes are a must for blue-green deployments**



Application Autoscaler
---------------------------------------------------------
to get service info from a service

Example -->cf m -s app-autoscaler

there are 2 ways of managing autoscaling
------------------------------------------
1)CPU thresholds
2)Schedule based--> example for business hours

12Factor----> disposibility

If we manually autoscale even though the autoscaler is bound to the application
then it will automatically unbind itself from the application and the application
will go back to manual scaling settings

Application Performance Monitor
------------------------------------------------------------------------------
example--> new relic & app dynamics

we create a new user-provided-service and bind it to the articulate application
In this case we need to restage because the droplet needs to be rebuilt so that the newrelic agent 
can be included inside of the droplet and run with our application
We should bind both the articulate application and attendee-service service to newrelic service
instance as well

*command to create a new relic service*
---------------------------------------
cf create-user-provided-service newrelic -p license_key

then we to need to input the license_key provided by newrelic to us

PCF Metrics
---------------------------------------------------------
what does spike in memory indicate when using pcf metrics
--> might mean a resource leak in the code


Buildpacks
--------------------------------------------------------------------

A droplet has the application, app server, libraries(application performance 
monitoring libraries), runtime

Buildpack API has three scripts 
1) bin/detect ---> determines whether the buildpack can stage the application
2) bin/compile ---> builds the droplet
3) bin/release ---> provides information on how to run the application

While pushing an application if only ****cf push**** is used then bin/detect is called 
and it iterates over all the buildpacks until one of them returns exit code 0

bin/detect is not called
------------------------------
Whereas if we specify the buildpack by using the command ****cf push -b buildpack-name****
the detect script is not going to be called

compile script
---------------------------------
provide any of the following if needed
1)runtime ---> java vm, ruby interpreter
2)app server  ---> tomcat
3)support libraries ---> npm packages

application performance monitor agents--> the buildpack will add it

release script
--------------------------------------
this outputs a yaml document 

commands
---------------------------------------------
cf buildpacks

to use custom buildpacks
-------------------------
cf push articulate -p ./articulate-0.2.jar -b https://github.com/cloudfoundry/java-buildpack.git

if we need our application to work in a specific version of the environment(for example for java)
---------------------------------------------------------------------------------------------------
cf set-env articulate JBP_CONFIG_OPEN_JDK_JRE "{jre:{version:1.8.0_45}}"

we can do the above for Tomcat as well i.e., if we need a specific version of the tomcat server


we can use pivotal network to update buildpacks periodically
we can refresh the pcf environment for that as well
we can also use custom buildpacks


which script is responsible for building the droplet --> compile script
how do buildpack helps in managing cve or security issues--> instead of patching systems
	manually, simply upload the patch buildpacks and restage the applications
will cf restart be sufficient or not--> no, because the droplet needs to be rebuilt

because we set the config parameter to set the java version, it notifies that the environment
variable is available to the staging process 

What other items are easily customized with the java-buildpack-->tomcat version,jre implementations
	

Service Brokers
----------------------------------------------------------------------------
Managed services must implement the Service Broker API

Service Broker can live in PCF or outside

Service Broker API--> restful api over http--> the application(example attendee-service) 
			that implements it can live anywhere

different endpoints to be implemented
-------------------------------------------------------
1)catalog mgmt--> plans offered by the service
2) provision --> creating the service instance
3) deprovision --> delete the provisioned service
4) bind
5) unbind

Provision
--------------------
mysql service --> shared dbms, large vm with multiple dbms

Binding---> provide unique credentials per binding --> 
		multiple applications using the same service
		instance, provide them each their unique credentials


Lab
-------------------------
cf push mongo-service-broker -p ./cf-mongodb-service-broker.jar -m 1G --random-route

cf set-env mongodb-service-broker SERVICE_ID mongodb-service-broker-ss

cf set-env mongodb-service-broker SERVICE_NAME MongoDB-ss

cf set-env mongodb-service-broker PLAN_ID mongo-plan-ss

cf start mongodb-service-broker


Space scoped service broker
-------------------------------------
allows a developer to built up a service without requiring admin priviledges for 
deployment

cf create-service-broker will throw an error because it needs the following as arguments
service broker name
username
password
url 


--space-scoped flag if we want that to work in a targeted space


LAB
---------------------------
cf create-service-broker mongodb-service-broker-ss pivotal keepitsimple https://mongodb-service-broker-sweet-alligator.cfapps.io --space-scoped

cf service-brokers --> to get the list of brokers in one's space
cf service-access--> to get the list of service brokers developer has access-->none

cf m --> to check whether out broker is listed in the marketplace or not 


for provisioning/de provisioning ---> we need to implement ServicInstanceService 
			in Spring Cloud	throw ServiceBrokerException

for binding/unbinding ---> implement ServiceInstanceBindingService
		if binding exists throw ServiceInstanceBindingExistsException
		
		if doesn't exist create a database and return back to the cloud controller
		the response with credentials which will be persisted in the CC_DB
		and then exposed to the bound application via the VCAP_SERVICES
		as well as save the binding in the binding repository i.e., MongoDB database

To see can we use our service broker
-----------------------------------------
cf m


Recap
------------------
where do service broker need to be deployed??
--> anywhere the cloud controller can reach it via an https call

why provide unique credentials for binding?
--> best practice...unbinding doesn't affect other applications other than the application
being unbounded.

can a service broker support upgrade or a downgrade of a service??
-->upgrade support is offered.... 

the broker is supposed to respond to the cloud controller within how many seconds?
-->60 seconds

does provisioning have to done synchronously?
--> No. It can be done asynchronously. we will have to implement another endpoint though

do all services have to be bindable??
--> No


Continuous Delivery
------------------------------------------------------------------------------
continuous delivery != continuous deployment

continuous delivery --> app is ready to be deployed?

Strategies
--------------------
Do implement continuous integration 
Don't create environment specific packages
Externalize environment specific config
Don't assume existing processes are right



Jenkins
-------------------------------------------------------------------

what are the continuous delivery strategies??
1)continuous integration
2)don't create application environment specific packages
3)blue green deployement strategies

does continuous delivery mean continuous deployment?
--> no, continuous deployment means that every commit is directly pushed into production
	where as continuous delivery means that every change can be deployed to
	production at any time.
what are the benefits of continuous delivery?
--> reducing risks, or increasing feedbacks, providing reliable releases



Routing Services
-----------------------------------------------------------------
purpose --> provide transformation or processing of requests after they reach an application

1)authentication
2) Rate limiting --> incase we don't want our api to be overrun by requests from users
3) Logging --> to log all the requests coming in and to provide more verbose logging
4) Caching --> cache responses instead of calling the downstream applications

 




